{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6fab78be",
   "metadata": {},
   "source": [
    "# LLM Evaluation Demo\n",
    "\n",
    "In this notebook we show how to create custom evaluation tasks within the `lm-eval-harness` library, a standard tool as of mid-2023 for LLM quantitative evaluation.\n",
    "\n",
    "### Table of Contents\n",
    "1. Install libraries\n",
    "2. Creating custom HuggingFace `Dataset` objects\n",
    "3. Testing LM harness (custom task)\n",
    "   - Define custom task (check JSON file too!)\n",
    "   - Run evaluation on checkpoint (zero shot)\n",
    "4. Testing a real Elsevier gold set - Embase's ICSR snapshot (zero shot)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3c24a7c",
   "metadata": {},
   "source": [
    "## 1. Install libraries\n",
    "\n",
    "To make this notebook work, you need to create a Python 3.9+ `conda` environment and a Jupyter kernel. You need to clone the `lm-eval-harness` GitHub repo. \n",
    "Here we demonstrate basic installation. For advanced installations (include multilingual support for MT evaluation, include support for 4-, 8- and 16-bit quantized models), please check the repo\n",
    "\n",
    "   - ``https://github.com/EleutherAI/lm-evaluation-harness.git``\n",
    "\n",
    "for instructions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "7d695575",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install pytest\n",
    "# !pip install -e ../tools/lm-evaluation-harness\n",
    "# !pip install --upgrade --force-reinstall pandas # uncomment if it doesn't install properly "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86ac9389",
   "metadata": {},
   "source": [
    "## 2. Creating custom HuggingFace `Dataset` objects<a class=\"anchor\" id=\"s1\"></a>\n",
    "\n",
    "Here we demonstrate how to parse custom, JSON-formatted NLP gold standards, into a HuggingFace `Dataset` and `DatasetDict`, **locally**. HuggingFace expects by default corpora to be uploaded to their public hub (this is fine for open data, but not for proprietary data). `lm-eval-harness` operates with HuggingFace `Dataset` objects.\n",
    "\n",
    "The example data is located within  the `relevancy` dir, and it is only a stub, demonstrating the format. Other examples are treated similarly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "5d7c2eb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from datasets.builder import BuilderConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "da966584",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"../datasets/harness/relevancy\", # path to directory; the name of the directory will become the dataset name\n",
    "                       data_files= {\"train\"     : \"train.json\", \n",
    "                                    \"test\"      : \"test.json\", \n",
    "                                    \"validation\": \"validation.json\"},\n",
    "                       field=\"data\"\n",
    "                      )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "37e839fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['Relevancy', 'EntityText', 'Context'],\n",
       "        num_rows: 2\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['Relevancy', 'EntityText', 'Context'],\n",
       "        num_rows: 2\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['Relevancy', 'EntityText', 'Context'],\n",
       "        num_rows: 2\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "a6c5db2a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Relevancy': Value(dtype='string', id=None),\n",
       " 'EntityText': Value(dtype='string', id=None),\n",
       " 'Context': Value(dtype='string', id=None)}"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['test'].features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "5d904549",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_icsr = load_dataset(\"../datasets/harness/icsr\", # path to directory; the name of the directory will become the dataset name\n",
    "                       data_files= {\"test\"      : \"test.json\"},\n",
    "                       field=\"data\",\n",
    "                      )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "2b7c8ec9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    test: Dataset({\n",
       "        features: ['Abstract', 'Relevant'],\n",
       "        num_rows: 2440\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_icsr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "f6448aff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Abstract': Value(dtype='string', id=None),\n",
       " 'Relevant': Value(dtype='string', id=None)}"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_icsr['test'].features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb25d142",
   "metadata": {},
   "source": [
    "## 3. Testing LM harness (custom task)\n",
    "\n",
    "The trick here is to define a custom task class, that inherits from class `Task`. The purpose of this class is to parse a JSON gold set into a HuggingFace `Dataset`, and then to transform its test-set partititon into a list of LLM prompts.\n",
    "\n",
    "As our relevancy examples there are totally arbitrary, the expected final accuracy is 0. We do want to prevent exceptions and parsing errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "aacceca6-ac76-4e21-8127-a4a57bdfc79b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lead repo into to Python path if it didn't install properly\n",
    "import sys\n",
    "sys.path.insert(0,'../tools/lm-evaluation-harness')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "b282e784",
   "metadata": {},
   "outputs": [],
   "source": [
    "from lm_eval import tasks, utils, evaluator"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21f011c3",
   "metadata": {},
   "source": [
    "### Define custom task (check JSON file too!)\n",
    "\n",
    "This class reads our custom benchmark into a `DatasetDict`, and then transforms every row (in the relevancy example, a dictionary `{'EntityText':'<v1>', \n",
    "'Context':<v2>, 'Relevancy':<v3>}` into a prompt via the public methods `doc_to_text()` (inputs) and `doc_to_target()` (ouputs) below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "4b8f87fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from lm_eval.base import rf, Task\n",
    "from lm_eval.metrics import mean\n",
    "import datasets\n",
    "\n",
    "_CITATION = \"\"\"\n",
    "@article{DBLP:journals/biodb/AkhondiRSMTNISI19,\n",
    "  author       = {Saber A. Akhondi and\n",
    "                  Hinnerk Rey and\n",
    "                  Markus Schw{\\\"{o}}rer and\n",
    "                  Michael Maier and\n",
    "                  John P. Toomey and\n",
    "                  Heike Nau and\n",
    "                  Gabriele Ilchmann and\n",
    "                  Mark Sheehan and\n",
    "                  Matthias Irmer and\n",
    "                  Claudia Bobach and\n",
    "                  Marius A. Doornenbal and\n",
    "                  Michelle Gregory and\n",
    "                  Jan A. Kors},\n",
    "  title        = {Automatic identification of relevant chemical compounds from patents},\n",
    "  journal      = {Database J. Biol. Databases Curation},\n",
    "  volume       = {2019},\n",
    "  pages        = {baz001},\n",
    "  year         = {2019},\n",
    "  url          = {https://doi.org/10.1093/database/baz001},\n",
    "  doi          = {10.1093/database/baz001},\n",
    "  timestamp    = {Thu, 13 Aug 2020 12:41:41 +0200},\n",
    "  biburl       = {https://dblp.org/rec/journals/biodb/AkhondiRSMTNISI19.bib},\n",
    "  bibsource    = {dblp computer science bibliography, https://dblp.org}\n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "class Relevancy(Task):\n",
    "    VERSION = \"0.0.0\"\n",
    "    DATASET_PATH = \"../datasets/harness/relevancy\" # absolute or relative path to dataset\n",
    "    DATASET_NAME = 'default'\n",
    "    \n",
    "    def __init__(self, data_dir=None, cache_dir=None, download_mode=None):\n",
    "        \"\"\"\n",
    "        Override default constructor so that the dataset is correctly\n",
    "        parsed by HuggingFace into a DatasetDict\n",
    "        \"\"\"\n",
    "        self._training_docs = None\n",
    "        self._fewshot_docs = None\n",
    "        self.dataset = datasets.load_dataset(\n",
    "            path=self.DATASET_PATH,\n",
    "            name=self.DATASET_NAME,\n",
    "            data_dir=data_dir,\n",
    "            cache_dir=cache_dir,\n",
    "            download_mode=download_mode,\n",
    "            field=\"data\",\n",
    "        )\n",
    "\n",
    "    def has_training_docs(self):\n",
    "        return True\n",
    "\n",
    "    def has_validation_docs(self):\n",
    "        return True\n",
    "\n",
    "    def has_test_docs(self):\n",
    "        return True\n",
    "        \n",
    "    def training_docs(self):\n",
    "        if self.has_training_docs():\n",
    "            return self.dataset[\"train\"]\n",
    "\n",
    "    def validation_docs(self):\n",
    "        if self.has_validation_docs():\n",
    "            return self.dataset[\"validation\"]\n",
    "\n",
    "    def test_docs(self):\n",
    "        if self.has_test_docs():\n",
    "            return self.dataset[\"test\"]\n",
    "\n",
    "    def doc_to_text(self, doc):\n",
    "        # Do we *really*\n",
    "        # want to do it exactly as OA did?\n",
    "        return (\n",
    "            \"Is entity\" + doc[\"EntityText\"]\n",
    "            + \" relevant in this text: \"\n",
    "            + doc[\"Context\"] + \"? \"\n",
    "            + \"Return either True or False.\\nAnswer:\"\n",
    "        )\n",
    "\n",
    "    def should_decontaminate(self):\n",
    "        return True\n",
    "\n",
    "    def doc_to_decontamination_query(self, doc):\n",
    "        return doc[\"Context\"]\n",
    "\n",
    "    def doc_to_target(self, doc):\n",
    "        return \" \" + doc[\"Relevancy\"]\n",
    "        \n",
    "    def construct_requests(self, doc, ctx):\n",
    "        \"\"\"Uses RequestFactory to construct Requests and returns an iterable of\n",
    "        Requests which will be sent to the LM.\n",
    "\n",
    "        :param doc:\n",
    "            The document as returned from training_docs, validation_docs, or test_docs.\n",
    "        :param ctx: str\n",
    "            The context string, generated by fewshot_context. This includes the natural\n",
    "            language description, as well as the few shot examples, and the question\n",
    "            part of the document for `doc`.\n",
    "        \"\"\"\n",
    "        ll_true, x = rf.loglikelihood(ctx, \" True\")\n",
    "        ll_false, y = rf.loglikelihood(ctx, \" False\")\n",
    "        #print(ll_true, ll_false)\n",
    "        #print(x, y)\n",
    "        return ll_true, ll_false\n",
    "\n",
    "    def process_results(self, doc, results):\n",
    "        \"\"\"Take a single document and the LM results and evaluates, returning a\n",
    "        dict where keys are the names of submetrics and values are the values of\n",
    "        the metric for that one document\n",
    "\n",
    "        :param doc:\n",
    "            The document as returned from training_docs, validation_docs, or test_docs.\n",
    "        :param results:\n",
    "            The results of the requests created in construct_requests.\n",
    "        \"\"\"\n",
    "        print(results)\n",
    "        gold    = doc[\"Relevancy\"]\n",
    "        pred    = np.argmax(results) # Notice that this library relies on *single token* best-first decoding\n",
    "        labels  = {0: \"True\", 1: \"False\"}\n",
    "        pred    = \"{}\".format(labels[pred])\n",
    "        print(\"gold:{} pred:{}\".format(gold, pred))\n",
    "        return {\"acc\": pred == gold}\n",
    "\n",
    "    def aggregation(self):\n",
    "        \"\"\"\n",
    "        :returns: {str: [float] -> float}\n",
    "            A dictionary where keys are the names of submetrics and values are\n",
    "            functions that aggregate a list of metrics\n",
    "        \"\"\"\n",
    "        return {\"acc\": mean}\n",
    "\n",
    "    def higher_is_better(self):\n",
    "        \"\"\"\n",
    "        :returns: {str: bool}\n",
    "            A dictionary where keys are the names of submetrics and values are\n",
    "            whether a higher value of the submetric is better\n",
    "        \"\"\"\n",
    "        return {\"acc\": True}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "121980ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "task = Relevancy() # instantiate the class to parse the sample gold set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "659126ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['Context', 'EntityText', 'Relevancy'],\n",
       "        num_rows: 2\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['Context', 'EntityText', 'Relevancy'],\n",
       "        num_rows: 2\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['Context', 'EntityText', 'Relevancy'],\n",
       "        num_rows: 2\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "task.dataset # should return a DatasetDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "69cceddd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'default'"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "task.DATASET_NAME # should print 'relevancy'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3a0fea9",
   "metadata": {},
   "source": [
    "We need to add our custom task to the `lm-eval-harness` registry of known NLP tasks:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "e90fb7d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "tasks.TASK_REGISTRY[\"relevancy\"] = Relevancy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9c29875",
   "metadata": {},
   "source": [
    "For comparison, we also download HellaSwag data from HuggingFace:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "ffc648fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "hellaswag = tasks.hellaswag.HellaSwag()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "7c7ea780",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['ind', 'activity_label', 'ctx_a', 'ctx_b', 'ctx', 'endings', 'source_id', 'split', 'split_type', 'label'],\n",
       "        num_rows: 39905\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['ind', 'activity_label', 'ctx_a', 'ctx_b', 'ctx', 'endings', 'source_id', 'split', 'split_type', 'label'],\n",
       "        num_rows: 10003\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['ind', 'activity_label', 'ctx_a', 'ctx_b', 'ctx', 'endings', 'source_id', 'split', 'split_type', 'label'],\n",
       "        num_rows: 10042\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hellaswag.dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "818364fa",
   "metadata": {},
   "source": [
    "### 3.2 Run evaluation on checkpoint\n",
    "\n",
    "Here, we evaluate on a previously downloaded / cached LLM (`Opt-350M` from Meta). If you want to try a different one, use a HuggingFace remote model path vs a local path.\n",
    "\n",
    "**<u>Important Notice</u>:** Evaluation in this notebook works only on CUDA (due to some obscure bug in the evaluation code that I wan't able to fix), so don't run it on CPU-only hosts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "9359e27e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = \"facebook/galactica-125m\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "851ce554",
   "metadata": {},
   "source": [
    "We check the registry of available decoder-only transformer architectures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "222c1b2f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'hf': lm_eval.models.gpt2.HFLM,\n",
       " 'hf-causal': lm_eval.models.gpt2.HFLM,\n",
       " 'hf-causal-experimental': lm_eval.models.huggingface.AutoCausalLM,\n",
       " 'hf-seq2seq': lm_eval.models.huggingface.AutoSeq2SeqLM,\n",
       " 'gpt2': lm_eval.models.gpt2.HFLM,\n",
       " 'gpt3': lm_eval.models.gpt3.GPT3LM,\n",
       " 'anthropic': lm_eval.models.anthropic_llms.AnthropicLM,\n",
       " 'textsynth': lm_eval.models.textsynth.TextSynthLM,\n",
       " 'dummy': lm_eval.models.dummy.DummyLM}"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from lm_eval import models\n",
    "models.MODEL_REGISTRY"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93c072c7",
   "metadata": {},
   "source": [
    "We first demonstrate evaluation on the HellaSwag benchmark. Notice that we limit the test to 10 examples per batch. Evaluation over an entire corpus can take --even on GPU-- many minutes, unless you use a multi-GPU machine. Recall that run-time per example in transformers in quadratic in the size of input tokens (due to attention). Also, some benchmarks can be huge.\n",
    "\n",
    "`lm-eval-harness` relies on PyTest to run evaluations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "0ffba4b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device 'cpu'\n",
      "\u001b[1m============================= test session starts ==============================\u001b[0m\n",
      "platform linux -- Python 3.10.14, pytest-8.2.0, pluggy-1.5.0\n",
      "rootdir: /home/jovyan/LREC-2024-notebooks/tools/lm-evaluation-harness\n",
      "plugins: anyio-4.3.0\n",
      "collected 441 items / 440 deselected / 1 selected\n",
      "\n",
      "../tools/lm-evaluation-harness/tests/test_version_stable.py \u001b[32m.\u001b[0m\u001b[32m            [100%]\u001b[0m\n",
      "\n",
      "\u001b[33m=============================== warnings summary ===============================\u001b[0m\n",
      "tests/test_version_stable.py::test_versions_stable[hellaswag-HellaSwag]\n",
      "  /home/jovyan/workbench-shared-folder/anaconda3/envs/python-310-lrec/lib/python3.10/site-packages/datasets/load.py:1486: FutureWarning: The repository for hellaswag contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at https://hf.co/datasets/hellaswag\n",
      "  You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "  Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
      "    warnings.warn(\n",
      "\n",
      "-- Docs: https://docs.pytest.org/en/stable/how-to/capture-warnings.html\n",
      "\u001b[33m================= \u001b[32m1 passed\u001b[0m, \u001b[33m\u001b[1m440 deselected\u001b[0m, \u001b[33m\u001b[1m1 warning\u001b[0m\u001b[33m in 3.99s\u001b[0m\u001b[33m =================\u001b[0m\n",
      "Task: hellaswag; number of docs: 10042\n",
      "Task: hellaswag; document 0; context prompt (starting on next line):\n",
      "Personal Care and Style: How to increase breast size with a bra. Check your bra size. Wearing a bra that is too big will not make your breasts look larger. That is why it is important to wear the right size bra for you.\n",
      "(end of prompt on previous line)\n",
      "Requests: [Req_loglikelihood('Personal Care and Style: How to increase breast size with a bra. Check your bra size. Wearing a bra that is too big will not make your breasts look larger. That is why it is important to wear the right size bra for you.', ' You can visit a lingerie shop and have them measure you to help you fit a bra to your size, or measure yourself before you shop for a new bra to ensure that you get a good fit. Use a flexible tape measure, like one found in a sewing kit.')[0]\n",
      ", Req_loglikelihood('Personal Care and Style: How to increase breast size with a bra. Check your bra size. Wearing a bra that is too big will not make your breasts look larger. That is why it is important to wear the right size bra for you.', ' This is why it is important to keep your breasts under protection when in the shower and only wear bras that are larger than your breast size. If you are not wearing a bra, try wearing something that is a little bigger.')[0]\n",
      ", Req_loglikelihood('Personal Care and Style: How to increase breast size with a bra. Check your bra size. Wearing a bra that is too big will not make your breasts look larger. That is why it is important to wear the right size bra for you.', ' For a girl, a bra with a support strap will be easier for her, because most women are unable to pull through bra straps and bras that are too small will not be able to support breasts from side-to-side. Many bras have even been created that cover the breast side, and can be sent to other women in the world to make them look bigger.')[0]\n",
      ", Req_loglikelihood('Personal Care and Style: How to increase breast size with a bra. Check your bra size. Wearing a bra that is too big will not make your breasts look larger. That is why it is important to wear the right size bra for you.', ' Choose a color that is flattering to your breast type and specific event, in addition to those that make you uncomfortable. Look for sports bras made from natural material, such as spandex or lycra, as this is a more breathable bra.')[0]\n",
      "]\n",
      "Running loglikelihood requests\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]\n"
     ]
    }
   ],
   "source": [
    "results = evaluator.simple_evaluate(\n",
    "        model=\"hf-causal\",\n",
    "        model_args=\" pretrained=\" + model_path,\n",
    "        tasks=[\"hellaswag\"],\n",
    "        num_fewshot=0,\n",
    "        device=\"cpu\",\n",
    "        limit=5, # We limit the number of queries\n",
    "        check_integrity=\"store_true\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "712f5a06",
   "metadata": {},
   "source": [
    "The result table should spit out a value of 0.3:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "b23c670f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|  Task   |Version| Metric |Value|   |Stderr|\n",
      "|---------|------:|--------|----:|---|-----:|\n",
      "|hellaswag|      0|acc     |  0.2|±  |0.2000|\n",
      "|         |       |acc_norm|  0.6|±  |0.2449|\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(evaluator.make_table(results))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26fbc34c",
   "metadata": {},
   "source": [
    "We now evaluate over our custom benchmark:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "ef2e19a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device 'cpu'\n",
      "Task: relevancy; number of docs: 2\n",
      "Task: relevancy; document 0; context prompt (starting on next line):\n",
      "Is entity4 relevant in this text: -5.5? Return either True or False.\n",
      "Answer:\n",
      "(end of prompt on previous line)\n",
      "Requests: (Req_loglikelihood('Is entity4 relevant in this text: -5.5? Return either True or False.\\nAnswer:', ' True')[0]\n",
      ", Req_loglikelihood('Is entity4 relevant in this text: -5.5? Return either True or False.\\nAnswer:', ' False')[0]\n",
      ")\n",
      "Running loglikelihood requests\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-1.8439056873321533, -3.0800535678863525]\n",
      "gold:True pred:True\n",
      "[-2.043750047683716, -3.6137354373931885]\n",
      "gold:False pred:True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "results_r = evaluator.simple_evaluate(\n",
    "        model=\"hf-causal\",\n",
    "        model_args=\" pretrained=\" + model_path,\n",
    "        tasks=[\"relevancy\"],\n",
    "        device=\"cpu\",\n",
    "        num_fewshot=0,\n",
    "        limit=10,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "6fd41186",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|  Task   |Version|Metric|Value|   |Stderr|\n",
      "|---------|-------|------|----:|---|-----:|\n",
      "|relevancy|0.0.0  |acc   |  0.5|±  |   0.5|\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(evaluator.make_table(results_r))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fcb715d",
   "metadata": {},
   "source": [
    "Few shot evaluation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "f2eecfa4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device 'cpu'\n",
      "Task: relevancy; number of docs: 2\n",
      "Task: relevancy; document 0; context prompt (starting on next line):\n",
      "Is entity1 relevant in this text: 2.0? Return either True or False.\n",
      "Answer: False\n",
      "\n",
      "Is entity4 relevant in this text: -5.5? Return either True or False.\n",
      "Answer:\n",
      "(end of prompt on previous line)\n",
      "Requests: (Req_loglikelihood('Is entity1 relevant in this text: 2.0? Return either True or False.\\nAnswer: False\\n\\nIs entity4 relevant in this text: -5.5? Return either True or False.\\nAnswer:', ' True')[0]\n",
      ", Req_loglikelihood('Is entity1 relevant in this text: 2.0? Return either True or False.\\nAnswer: False\\n\\nIs entity4 relevant in this text: -5.5? Return either True or False.\\nAnswer:', ' False')[0]\n",
      ")\n",
      "Running loglikelihood requests\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.5465310215950012, -0.921859085559845]\n",
      "gold:True pred:True\n",
      "[-0.43615809082984924, -1.0989350080490112]\n",
      "gold:False pred:True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "results_few = evaluator.simple_evaluate(\n",
    "        model=\"hf-causal\",\n",
    "        model_args=\" pretrained=\" + model_path,\n",
    "        tasks=[\"relevancy\"],\n",
    "        num_fewshot=1,\n",
    "        device=\"cpu\",\n",
    "        limit=10,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "944a31b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|  Task   |Version|Metric|Value|   |Stderr|\n",
      "|---------|-------|------|----:|---|-----:|\n",
      "|relevancy|0.0.0  |acc   |  0.5|±  |   0.5|\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(evaluator.make_table(results_few))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34fc7876",
   "metadata": {},
   "source": [
    "## 4. Testing a real Elsevier gold set - Embase's ICSR snapshot\n",
    "\n",
    "We start by measuring accuracy, where we apply the formula:\n",
    "$$\n",
    "Acc = \\frac{TP}{TP + FP + FN + FP}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "77b5d302",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from lm_eval.base import rf, Task\n",
    "from lm_eval.metrics import mean\n",
    "import datasets\n",
    "\n",
    "_CITATION = \"\"\"\n",
    "NaN\n",
    "\"\"\"\n",
    "\n",
    "class ICSR(Task):\n",
    "    VERSION = \"0.1.0\"\n",
    "    DATASET_PATH = \"../datasets/harness/icsr\" # absolute or relative path to dataset\n",
    "    DATASET_NAME = \"default\"\n",
    "    \n",
    "    def __init__(self, data_dir=None, cache_dir=None, download_mode=None):\n",
    "        \"\"\"\n",
    "        Override default constructor so that the dataset is correctly\n",
    "        parsed by HuggingFace into a DatasetDict\n",
    "        \"\"\"\n",
    "        self._training_docs = None\n",
    "        self._fewshot_docs = None\n",
    "        self.dataset = datasets.load_dataset(\n",
    "            path=self.DATASET_PATH,\n",
    "            name=self.DATASET_NAME,\n",
    "            data_dir=data_dir,\n",
    "            cache_dir=cache_dir,\n",
    "            download_mode=download_mode,\n",
    "            field=\"data\",\n",
    "        )\n",
    "\n",
    "    def has_training_docs(self):\n",
    "        return True\n",
    "\n",
    "    def has_validation_docs(self):\n",
    "        return True\n",
    "\n",
    "    def has_test_docs(self):\n",
    "        return True\n",
    "        \n",
    "    def training_docs(self):\n",
    "        if self.has_training_docs():\n",
    "            return self.dataset[\"train\"]\n",
    "\n",
    "    def validation_docs(self):\n",
    "        if self.has_validation_docs():\n",
    "            return self.dataset[\"validation\"]\n",
    "\n",
    "    def test_docs(self):\n",
    "        if self.has_test_docs():\n",
    "            return self.dataset[\"test\"]\n",
    "\n",
    "    def doc_to_text(self, doc):\n",
    "        # Do we *really*\n",
    "        # want to do it exactly as OA did?\n",
    "        return (\n",
    "            \"Does this paper abstract: \" + doc[\"Abstract\"]\n",
    "            + \" mention adverse drug effects? \"\n",
    "            + \"Please answer Yes or No.\\nAnswer:\"\n",
    "        )\n",
    "\n",
    "    def should_decontaminate(self):\n",
    "        return True\n",
    "\n",
    "    def doc_to_decontamination_query(self, doc):\n",
    "        return doc[\"Abstract\"]\n",
    "        \n",
    "    def doc_to_target(self, doc):\n",
    "        return \" \" + doc[\"Abstract\"]\n",
    "\n",
    "    def construct_requests(self, doc, ctx):\n",
    "        \"\"\"Uses RequestFactory to construct Requests and returns an iterable of\n",
    "        Requests which will be sent to the LM.\n",
    "\n",
    "        :param doc:\n",
    "            The document as returned from training_docs, validation_docs, or test_docs.\n",
    "        :param ctx: str\n",
    "            The context string, generated by fewshot_context. This includes the natural\n",
    "            language description, as well as the few shot examples, and the question\n",
    "            part of the document for `doc`.\n",
    "        \"\"\"\n",
    "        ll_true, _ = rf.loglikelihood(ctx, \" Yes\")\n",
    "        ll_false, _ = rf.loglikelihood(ctx, \" No\")\n",
    "        return ll_true, ll_false\n",
    "\n",
    "    def process_results(self, doc, results):\n",
    "        \"\"\"Take a single document and the LM results and evaluates, returning a\n",
    "        dict where keys are the names of submetrics and values are the values of\n",
    "        the metric for that one document\n",
    "\n",
    "        :param doc:\n",
    "            The document as returned from training_docs, validation_docs, or test_docs.\n",
    "        :param results:\n",
    "            The results of the requests created in construct_requests.\n",
    "        \"\"\"\n",
    "        gold    = doc[\"Relevant\"]\n",
    "        pred    = np.argmax(results) # Notice that this library relies on *single token* best-first decoding\n",
    "        labels  = {0: \"Yes\", 1: \"No\"}\n",
    "        pred    = \"{}\".format(labels[pred])\n",
    "        return {\"acc\": pred == gold}\n",
    "\n",
    "    def aggregation(self):\n",
    "        \"\"\"\n",
    "        :returns: {str: [float] -> float}\n",
    "            A dictionary where keys are the names of submetrics and values are\n",
    "            functions that aggregate a list of metrics\n",
    "        \"\"\"\n",
    "        return {\"acc\": mean}\n",
    "\n",
    "    def higher_is_better(self):\n",
    "        \"\"\"\n",
    "        :returns: {str: bool}\n",
    "            A dictionary where keys are the names of submetrics and values are\n",
    "            whether a higher value of the submetric is better\n",
    "        \"\"\"\n",
    "        return {\"acc\": True}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "f042cac2",
   "metadata": {},
   "outputs": [],
   "source": [
    "task_icsr = ICSR()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "4724ce2b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    test: Dataset({\n",
       "        features: ['Relevant', 'Abstract'],\n",
       "        num_rows: 2440\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "task_icsr.dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "e8329984",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Relevant': 'Yes',\n",
       " 'Abstract': 'Aims/Introduction: The convergence of tuberculosis (TB) and diabetes mellitus (DM) is a new challenge in Asia as a result of the rising prevalence of diabetes mellitus with higher TB infection rates, and also because diabetes mellitus itself enhances TB disease activity and consequently the spread of TB. We aimed to address the risk presented by diabetes mellitus for TB infection. Materials and Methods: Patients with diabetes mellitus were retrospectively recruited. The baseline assessments included age, sex, body mass index, fasting blood glucose, glycated hemoglobin, urine albumin-to-creatinine ratio and estimated glomerular filtration rate. TB was determined by meeting the international classification of disease, for TB diagNosis and receiving anti-TB treatment for at least 2¬†months. Results: In total, 9,750 individuals with diabetes mellitus were recruited. The event rate of TB was 47 (0.48%). Younger age, lower proportion of men, higher fasting blood glucose and glycated hemoglobin values, and better renal function (estimated glomerular filtration rate and urine albumin-to-creatinine ratio) were observed in the metformin-exposed groups. Old age and male sex were associated with higher TB infection risk on multivariate analysis. Metformin users had a significantly lower risk for TB infection, whereas insulin users had a higher risk for TB infection. However, glycemic status had Noeffect on TB infection risk. Conclusions: This study provides clinical evidence from a survey of TB in individuals with diabetes mellitus. Old age, male sex and insulin use were risk factors for TB infection. Metformin remains the first choice of treatment for diabetes mellitus and has a potential protective effect against TB infection.'}"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "task_icsr.dataset['test'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "c6e9dc5a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'default'"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "task_icsr.DATASET_NAME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "9b38b3d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "tasks.TASK_REGISTRY[\"icsr\"] = ICSR"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61d71342",
   "metadata": {},
   "source": [
    "Run zero-shot eval on ICSR (should return accuracy > 0):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "a214c352",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device 'cpu'\n",
      "Task: icsr; number of docs: 2440\n",
      "Task: icsr; document 0; context prompt (starting on next line):\n",
      "Does this paper abstract: Objectives The aims of this study were to describe the following: (1) the time to change of therapy in patients with type 2 diabetes who had initiated metformin moNotherapy as first-line treatment and (2) the sequence in which subsequent therapeutic regimens were introduced. Design Cohort study. Setting National study based on linked data from the New Zealand Ministry of Health's National Collections of health and pharmaceutical dispensing data. Participants People with type 2 diabetes mellitus who initiated metformin moNotherapy between 1 January 2006 and 30 September 2014 (n=93 874). Primary outcome measures Cumulative incidence curves were plotted to show the time taken to move from one regimen to aNother, while sunburst plots were used to illustrate the sequence in which regimens were introduced. Results About 10% and 35% of cohort members had moved to a second regimen 1 year and 5 years, respectively, after initiating metformin moNotherapy; the majority received a regimen recommended by New Zealand treatment guidelines (mostly metformin and a sulphonylurea). Of those who started a recommended second regimen, 37% and 67% had moved to a third regimen after 1 and 5 years, respectively; the corresponding proportions for those who started an √¢ ‚Ç¨ other' (Not listed as recommended) second regimen were 53% and 75%. Most of those who received a third regimen after a recommended second regimen were dispensed an √¢ ‚Ç¨ other' third regimen. Of those who moved to a third regimen from an √¢ ‚Ç¨ other' second regimen, similar proportions received recommended and √¢ ‚Ç¨ other' third regimens. Conclusions Real-world type 2 diabetes treatment patterns in New Zealand are complex and Not always consistent with guidelines. mention adverse drug effects? Please answer Yes or No.\n",
      "Answer:\n",
      "(end of prompt on previous line)\n",
      "Requests: (Req_loglikelihood(\"Does this paper abstract: Objectives The aims of this study were to describe the following: (1) the time to change of therapy in patients with type 2 diabetes who had initiated metformin moNotherapy as first-line treatment and (2) the sequence in which subsequent therapeutic regimens were introduced. Design Cohort study. Setting National study based on linked data from the New Zealand Ministry of Health's National Collections of health and pharmaceutical dispensing data. Participants People with type 2 diabetes mellitus who initiated metformin moNotherapy between 1 January 2006 and 30 September 2014 (n=93 874). Primary outcome measures Cumulative incidence curves were plotted to show the time taken to move from one regimen to aNother, while sunburst plots were used to illustrate the sequence in which regimens were introduced. Results About 10% and 35% of cohort members had moved to a second regimen 1 year and 5 years, respectively, after initiating metformin moNotherapy; the majority received a regimen recommended by New Zealand treatment guidelines (mostly metformin and a sulphonylurea). Of those who started a recommended second regimen, 37% and 67% had moved to a third regimen after 1 and 5 years, respectively; the corresponding proportions for those who started an √¢ ‚Ç¨ other' (Not listed as recommended) second regimen were 53% and 75%. Most of those who received a third regimen after a recommended second regimen were dispensed an √¢ ‚Ç¨ other' third regimen. Of those who moved to a third regimen from an √¢ ‚Ç¨ other' second regimen, similar proportions received recommended and √¢ ‚Ç¨ other' third regimens. Conclusions Real-world type 2 diabetes treatment patterns in New Zealand are complex and Not always consistent with guidelines. mention adverse drug effects? Please answer Yes or No.\\nAnswer:\", ' Yes')[0]\n",
      ", Req_loglikelihood(\"Does this paper abstract: Objectives The aims of this study were to describe the following: (1) the time to change of therapy in patients with type 2 diabetes who had initiated metformin moNotherapy as first-line treatment and (2) the sequence in which subsequent therapeutic regimens were introduced. Design Cohort study. Setting National study based on linked data from the New Zealand Ministry of Health's National Collections of health and pharmaceutical dispensing data. Participants People with type 2 diabetes mellitus who initiated metformin moNotherapy between 1 January 2006 and 30 September 2014 (n=93 874). Primary outcome measures Cumulative incidence curves were plotted to show the time taken to move from one regimen to aNother, while sunburst plots were used to illustrate the sequence in which regimens were introduced. Results About 10% and 35% of cohort members had moved to a second regimen 1 year and 5 years, respectively, after initiating metformin moNotherapy; the majority received a regimen recommended by New Zealand treatment guidelines (mostly metformin and a sulphonylurea). Of those who started a recommended second regimen, 37% and 67% had moved to a third regimen after 1 and 5 years, respectively; the corresponding proportions for those who started an √¢ ‚Ç¨ other' (Not listed as recommended) second regimen were 53% and 75%. Most of those who received a third regimen after a recommended second regimen were dispensed an √¢ ‚Ç¨ other' third regimen. Of those who moved to a third regimen from an √¢ ‚Ç¨ other' second regimen, similar proportions received recommended and √¢ ‚Ç¨ other' third regimens. Conclusions Real-world type 2 diabetes treatment patterns in New Zealand are complex and Not always consistent with guidelines. mention adverse drug effects? Please answer Yes or No.\\nAnswer:\", ' No')[0]\n",
      ")\n",
      "Running loglikelihood requests\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]\n"
     ]
    }
   ],
   "source": [
    "results_icsr = evaluator.simple_evaluate(\n",
    "        model=\"hf-causal\",\n",
    "        model_args=\" pretrained=\" + model_path,\n",
    "        tasks=[\"icsr\"],\n",
    "        num_fewshot=0,\n",
    "        device=\"cpu\",\n",
    "        limit=10,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "ebb9b296",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|Task|Version|Metric|Value|   |Stderr|\n",
      "|----|-------|------|----:|---|-----:|\n",
      "|icsr|0.1.0  |acc   |  0.4|±  |0.1633|\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(evaluator.make_table(results_icsr)) # 22.3% of all labels are 'Yes'!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d02b4815",
   "metadata": {},
   "source": [
    "## 4. Measure (word) perplexity\n",
    "\n",
    "See:\n",
    "- https://towardsdatascience.com/perplexity-of-language-models-revisited-6b9b4cf46792\n",
    "- https://en.wikipedia.org/wiki/Perplexity\n",
    "- https://huggingface.co/docs/transformers/perplexity\n",
    "\n",
    "#### Definition 1\n",
    "Given a vocabulary $V$ of size $|V|$ and tokens $t \\in V$. \n",
    "Given a corpus $D \\subseteq V^*$ of size $|D|$ and sentences $s \\in D$ (every sentence is a sequences over V), each of\n",
    "probability $p(s)$.\n",
    "The perplexity of probability distribution $p$ on $D$ is defined as:\n",
    "$$\n",
    "\\begin{align}\n",
    "PPL(p) & ~~=~~ (\\prod_{i=1}^{|D|} p(s_i))^{-\\frac{1}{|V|}}\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "#### Definition 2\n",
    "\n",
    "This definition defines perpexlity as a function of the cross entropy $CE(\\cdot,\\cdot)$ of the probability distribution $p$ that \n",
    "we assume generated corpus $D$ (viz. $D \\sim p$),\n",
    "and the probability distribution $q_{\\theta}$ estimated by model $\\theta$ from $D$:\n",
    "$$\n",
    "\\begin{align}\n",
    "PPL(p,q_{\\theta}) & ~~=~~       2^{CE(p, q_{\\theta})} \\\\\n",
    "                  & ~~\\approx~~ 2^{ - \\frac{1}{t} \\sum_{i=1}^{t} \\log q(t_{i}|t_{1},\\dots,t_{i-1}) }\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "fab3867a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datasets\n",
    "\n",
    "from lm_eval.base import PerplexityTask\n",
    "from lm_eval.utils import escaped_split\n",
    "\n",
    "class ICSRPerplexity(PerplexityTask):\n",
    "    '''\n",
    "    We re-use here the ICSR data, but we throw away the labels.\n",
    "    We ask the model to generate the abstracts, and measure LM perplexity.\n",
    "    '''\n",
    "    VERSION = \"0.1.0\"\n",
    "    DATASET_PATH = \"../datasets/harness/icsr\"\n",
    "    DATASET_NAME = \"default\"\n",
    "\n",
    "    def __init__(self, data_dir=None, cache_dir=None, download_mode=None):\n",
    "        self.data_dir = data_dir\n",
    "        self.dataset = datasets.load_dataset(\n",
    "            path=self.DATASET_PATH,\n",
    "            name=self.DATASET_NAME,\n",
    "            data_dir=data_dir,\n",
    "            cache_dir=cache_dir,\n",
    "            download_mode=download_mode,\n",
    "            field=\"data\",\n",
    "        )\n",
    "        self._training_docs = None\n",
    "        self._fewshot_docs = None\n",
    "\n",
    "    def download(self, data_dir=None, cache_dir=None, download_mode=None):\n",
    "        raise TypeError(\"cannot download an arbitrary JSON dataset\")\n",
    "\n",
    "    def has_validation_docs(self):\n",
    "        return False\n",
    "    \n",
    "    def has_training_docs(self):\n",
    "        return False\n",
    "\n",
    "    def has_test_docs(self):\n",
    "        return True\n",
    "\n",
    "    def test_docs(self):\n",
    "        return map(self._process_doc, self.dataset[\"test\"])\n",
    "\n",
    "    def _process_doc(self, doc):\n",
    "        return doc['Abstract']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "e8842f90",
   "metadata": {},
   "outputs": [],
   "source": [
    "perp_task = ICSRPerplexity()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "98ec51ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "tasks.TASK_REGISTRY[\"icsr_perplexity\"] = ICSRPerplexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "cc23fdf3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device 'cpu'\n",
      "Task: icsr_perplexity; number of docs: 2440\n",
      "Task: icsr_perplexity; document 0; context prompt (starting on next line):\n",
      "\n",
      "(end of prompt on previous line)\n",
      "Requests: Req_loglikelihood_rolling(\"Objectives The aims of this study were to describe the following: (1) the time to change of therapy in patients with type 2 diabetes who had initiated metformin moNotherapy as first-line treatment and (2) the sequence in which subsequent therapeutic regimens were introduced. Design Cohort study. Setting National study based on linked data from the New Zealand Ministry of Health's National Collections of health and pharmaceutical dispensing data. Participants People with type 2 diabetes mellitus who initiated metformin moNotherapy between 1 January 2006 and 30 September 2014 (n=93 874). Primary outcome measures Cumulative incidence curves were plotted to show the time taken to move from one regimen to aNother, while sunburst plots were used to illustrate the sequence in which regimens were introduced. Results About 10% and 35% of cohort members had moved to a second regimen 1 year and 5 years, respectively, after initiating metformin moNotherapy; the majority received a regimen recommended by New Zealand treatment guidelines (mostly metformin and a sulphonylurea). Of those who started a recommended second regimen, 37% and 67% had moved to a third regimen after 1 and 5 years, respectively; the corresponding proportions for those who started an √¢ ‚Ç¨ other' (Not listed as recommended) second regimen were 53% and 75%. Most of those who received a third regimen after a recommended second regimen were dispensed an √¢ ‚Ç¨ other' third regimen. Of those who moved to a third regimen from an √¢ ‚Ç¨ other' second regimen, similar proportions received recommended and √¢ ‚Ç¨ other' third regimens. Conclusions Real-world type 2 diabetes treatment patterns in New Zealand are complex and Not always consistent with guidelines.\",)[None]\n",
      "\n",
      "Running loglikelihood_rolling requests\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'NoneType' object cannot be interpreted as an integer",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[132], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m results_p \u001b[38;5;241m=\u001b[39m \u001b[43mevaluator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msimple_evaluate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mhf-causal\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel_args\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m pretrained=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mmodel_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtasks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43micsr_perplexity\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnum_fewshot\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcpu\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlimit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m# We limit the number of queries\u001b[39;49;00m\n\u001b[1;32m      8\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m#check_integrity=\"store_true\",\u001b[39;49;00m\n\u001b[1;32m      9\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/LREC-2024-notebooks/notebooks/../tools/lm-evaluation-harness/lm_eval/utils.py:243\u001b[0m, in \u001b[0;36mpositional_deprecated.<locals>._wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    237\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(args) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m inspect\u001b[38;5;241m.\u001b[39mismethod(fn) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    238\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\n\u001b[1;32m    239\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWARNING: using \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfn\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m with positional arguments is \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    240\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdeprecated and will be disallowed in a future version of \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    241\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlm-evaluation-harness!\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    242\u001b[0m     )\n\u001b[0;32m--> 243\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/LREC-2024-notebooks/notebooks/../tools/lm-evaluation-harness/lm_eval/evaluator.py:105\u001b[0m, in \u001b[0;36msimple_evaluate\u001b[0;34m(model, model_args, tasks, num_fewshot, batch_size, max_batch_size, device, no_cache, limit, bootstrap_iters, description_dict, check_integrity, decontamination_ngrams_path, write_out, output_base_path)\u001b[0m\n\u001b[1;32m    102\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m check_integrity:\n\u001b[1;32m    103\u001b[0m     run_task_tests(task_list\u001b[38;5;241m=\u001b[39mtasks)\n\u001b[0;32m--> 105\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    106\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlm\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    107\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtask_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtask_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    108\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_fewshot\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_fewshot\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    109\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlimit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlimit\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    110\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbootstrap_iters\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbootstrap_iters\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    111\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdescription_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdescription_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    112\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdecontamination_ngrams_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecontamination_ngrams_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    113\u001b[0m \u001b[43m    \u001b[49m\u001b[43mwrite_out\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwrite_out\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    114\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_base_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_base_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    115\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    117\u001b[0m \u001b[38;5;66;03m# add info about the model and few shot config\u001b[39;00m\n\u001b[1;32m    118\u001b[0m model_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/LREC-2024-notebooks/notebooks/../tools/lm-evaluation-harness/lm_eval/utils.py:243\u001b[0m, in \u001b[0;36mpositional_deprecated.<locals>._wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    237\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(args) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m inspect\u001b[38;5;241m.\u001b[39mismethod(fn) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    238\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\n\u001b[1;32m    239\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWARNING: using \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfn\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m with positional arguments is \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    240\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdeprecated and will be disallowed in a future version of \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    241\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlm-evaluation-harness!\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    242\u001b[0m     )\n\u001b[0;32m--> 243\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/LREC-2024-notebooks/notebooks/../tools/lm-evaluation-harness/lm_eval/evaluator.py:305\u001b[0m, in \u001b[0;36mevaluate\u001b[0;34m(lm, task_dict, provide_description, num_fewshot, limit, bootstrap_iters, description_dict, decontamination_ngrams_path, write_out, output_base_path)\u001b[0m\n\u001b[1;32m    298\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m reqtype, reqs \u001b[38;5;129;01min\u001b[39;00m requests\u001b[38;5;241m.\u001b[39mitems():\n\u001b[1;32m    299\u001b[0m     \u001b[38;5;66;03m# TODO: right now, this code runs multiple separate LM requests for multiple Requests differing\u001b[39;00m\n\u001b[1;32m    300\u001b[0m     \u001b[38;5;66;03m#       only in index. We could implement some kind of caching, but that would be more of a band-aid\u001b[39;00m\n\u001b[1;32m    301\u001b[0m     \u001b[38;5;66;03m#       solution. we could also implement some kind of auto-grouping here;\u001b[39;00m\n\u001b[1;32m    302\u001b[0m     \u001b[38;5;66;03m#       they should end up next to each other.\u001b[39;00m\n\u001b[1;32m    304\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRunning\u001b[39m\u001b[38;5;124m\"\u001b[39m, reqtype, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrequests\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 305\u001b[0m     resps \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mlm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreqtype\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mreq\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mreq\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mreqs\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    306\u001b[0m     resps \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    307\u001b[0m         x \u001b[38;5;28;01mif\u001b[39;00m req\u001b[38;5;241m.\u001b[39mindex \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m x[req\u001b[38;5;241m.\u001b[39mindex] \u001b[38;5;28;01mfor\u001b[39;00m x, req \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(resps, reqs)\n\u001b[1;32m    308\u001b[0m     ]\n\u001b[1;32m    310\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m resp, (i, task_name, doc, doc_id) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(resps, requests_origin[reqtype]):\n",
      "File \u001b[0;32m~/LREC-2024-notebooks/notebooks/../tools/lm-evaluation-harness/lm_eval/base.py:922\u001b[0m, in \u001b[0;36mCachingLM.__getattr__.<locals>.fn\u001b[0;34m(requests)\u001b[0m\n\u001b[1;32m    919\u001b[0m         remaining_reqs\u001b[38;5;241m.\u001b[39mappend(req)\n\u001b[1;32m    921\u001b[0m \u001b[38;5;66;03m# actually run the LM on the requests that do not have cached results\u001b[39;00m\n\u001b[0;32m--> 922\u001b[0m rem_res \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattr\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43mremaining_reqs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    924\u001b[0m \u001b[38;5;66;03m# stick the new ones back into the list and also cache any of the new ones\u001b[39;00m\n\u001b[1;32m    925\u001b[0m resptr \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n",
      "File \u001b[0;32m~/LREC-2024-notebooks/notebooks/../tools/lm-evaluation-harness/lm_eval/base.py:257\u001b[0m, in \u001b[0;36mBaseLM.loglikelihood_rolling\u001b[0;34m(self, requests)\u001b[0m\n\u001b[1;32m    253\u001b[0m rolling_token_windows \u001b[38;5;241m=\u001b[39m [(\u001b[38;5;28;01mNone\u001b[39;00m,) \u001b[38;5;241m+\u001b[39m x \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m rolling_token_windows]\n\u001b[1;32m    255\u001b[0m \u001b[38;5;66;03m# TODO: extract out this call so it only gets called once and also somehow figure out partial caching for\u001b[39;00m\n\u001b[1;32m    256\u001b[0m \u001b[38;5;66;03m# that\u001b[39;00m\n\u001b[0;32m--> 257\u001b[0m string_nll \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_loglikelihood_tokens\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    258\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrolling_token_windows\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    259\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdisable_tqdm\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    260\u001b[0m \u001b[43m    \u001b[49m\u001b[43moverride_bs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43madaptive_batch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    261\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    263\u001b[0m \u001b[38;5;66;03m# discard is_greedy\u001b[39;00m\n\u001b[1;32m    264\u001b[0m string_nll \u001b[38;5;241m=\u001b[39m [x[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m string_nll]\n",
      "File \u001b[0;32m~/LREC-2024-notebooks/notebooks/../tools/lm-evaluation-harness/lm_eval/base.py:339\u001b[0m, in \u001b[0;36mBaseLM._loglikelihood_tokens\u001b[0;34m(self, requests, disable_tqdm, override_bs)\u001b[0m\n\u001b[1;32m    329\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(continuation_enc) \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_length\n\u001b[1;32m    331\u001b[0m \u001b[38;5;66;03m# how this all works:\u001b[39;00m\n\u001b[1;32m    332\u001b[0m \u001b[38;5;66;03m#          CTX      CONT\u001b[39;00m\n\u001b[1;32m    333\u001b[0m \u001b[38;5;66;03m# inp    0 1 2 3|4 5 6 7 8 9   <- last token is deleted by inp[:, :-1]\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    337\u001b[0m \n\u001b[1;32m    338\u001b[0m \u001b[38;5;66;03m# when too long to fit in context, truncate from the left\u001b[39;00m\n\u001b[0;32m--> 339\u001b[0m inp \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtensor\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    340\u001b[0m \u001b[43m    \u001b[49m\u001b[43m(\u001b[49m\u001b[43mcontext_enc\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mcontinuation_enc\u001b[49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_length\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    341\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlong\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    342\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m    343\u001b[0m (inplen,) \u001b[38;5;241m=\u001b[39m inp\u001b[38;5;241m.\u001b[39mshape\n\u001b[1;32m    345\u001b[0m cont \u001b[38;5;241m=\u001b[39m continuation_enc\n",
      "\u001b[0;31mTypeError\u001b[0m: 'NoneType' object cannot be interpreted as an integer"
     ]
    }
   ],
   "source": [
    "results_p = evaluator.simple_evaluate(\n",
    "        model=\"hf-causal\",\n",
    "        model_args=\" pretrained=\" + model_path,\n",
    "        tasks=[\"icsr_perplexity\"],\n",
    "        num_fewshot=0,\n",
    "        device='cpu',\n",
    "        limit=10, # We limit the number of queries\n",
    "        #check_integrity=\"store_true\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53659faa",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(evaluator.make_table(results_p))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5ae2ed2",
   "metadata": {},
   "source": [
    "## 5. Repeat with a different model\n",
    "\n",
    "We repeat zero-shot and perplexity experiments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "afc11ea8",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path_new = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "42057a4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device 'cuda'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset json (/home/jovyan/.cache/huggingface/datasets/json/icsr-2e3e9176217a300b/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)\n",
      "100%|██████████| 1/1 [00:00<00:00, 257.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Task: icsr; number of docs: 2440\n",
      "Task: icsr; document 0; context prompt (starting on next line):\n",
      "Does this paper abstract: Objectives The aims of this study were to describe the following: (1) the time to change of therapy in patients with type 2 diabetes who had initiated metformin moNotherapy as first-line treatment and (2) the sequence in which subsequent therapeutic regimens were introduced. Design Cohort study. Setting National study based on linked data from the New Zealand Ministry of Health's National Collections of health and pharmaceutical dispensing data. Participants People with type 2 diabetes mellitus who initiated metformin moNotherapy between 1 January 2006 and 30 September 2014 (n=93 874). Primary outcome measures Cumulative incidence curves were plotted to show the time taken to move from one regimen to aNother, while sunburst plots were used to illustrate the sequence in which regimens were introduced. Results About 10% and 35% of cohort members had moved to a second regimen 1 year and 5 years, respectively, after initiating metformin moNotherapy; the majority received a regimen recommended by New Zealand treatment guidelines (mostly metformin and a sulphonylurea). Of those who started a recommended second regimen, 37% and 67% had moved to a third regimen after 1 and 5 years, respectively; the corresponding proportions for those who started an √¢ ‚Ç¨ other' (Not listed as recommended) second regimen were 53% and 75%. Most of those who received a third regimen after a recommended second regimen were dispensed an √¢ ‚Ç¨ other' third regimen. Of those who moved to a third regimen from an √¢ ‚Ç¨ other' second regimen, similar proportions received recommended and √¢ ‚Ç¨ other' third regimens. Conclusions Real-world type 2 diabetes treatment patterns in New Zealand are complex and Not always consistent with guidelines. mention adverse drug effects? Please answer Yes or No.\n",
      "Answer:\n",
      "(end of prompt on previous line)\n",
      "Requests: (Req_loglikelihood(\"Does this paper abstract: Objectives The aims of this study were to describe the following: (1) the time to change of therapy in patients with type 2 diabetes who had initiated metformin moNotherapy as first-line treatment and (2) the sequence in which subsequent therapeutic regimens were introduced. Design Cohort study. Setting National study based on linked data from the New Zealand Ministry of Health's National Collections of health and pharmaceutical dispensing data. Participants People with type 2 diabetes mellitus who initiated metformin moNotherapy between 1 January 2006 and 30 September 2014 (n=93 874). Primary outcome measures Cumulative incidence curves were plotted to show the time taken to move from one regimen to aNother, while sunburst plots were used to illustrate the sequence in which regimens were introduced. Results About 10% and 35% of cohort members had moved to a second regimen 1 year and 5 years, respectively, after initiating metformin moNotherapy; the majority received a regimen recommended by New Zealand treatment guidelines (mostly metformin and a sulphonylurea). Of those who started a recommended second regimen, 37% and 67% had moved to a third regimen after 1 and 5 years, respectively; the corresponding proportions for those who started an √¢ ‚Ç¨ other' (Not listed as recommended) second regimen were 53% and 75%. Most of those who received a third regimen after a recommended second regimen were dispensed an √¢ ‚Ç¨ other' third regimen. Of those who moved to a third regimen from an √¢ ‚Ç¨ other' second regimen, similar proportions received recommended and √¢ ‚Ç¨ other' third regimens. Conclusions Real-world type 2 diabetes treatment patterns in New Zealand are complex and Not always consistent with guidelines. mention adverse drug effects? Please answer Yes or No.\\nAnswer:\", ' Yes')[0]\n",
      ", Req_loglikelihood(\"Does this paper abstract: Objectives The aims of this study were to describe the following: (1) the time to change of therapy in patients with type 2 diabetes who had initiated metformin moNotherapy as first-line treatment and (2) the sequence in which subsequent therapeutic regimens were introduced. Design Cohort study. Setting National study based on linked data from the New Zealand Ministry of Health's National Collections of health and pharmaceutical dispensing data. Participants People with type 2 diabetes mellitus who initiated metformin moNotherapy between 1 January 2006 and 30 September 2014 (n=93 874). Primary outcome measures Cumulative incidence curves were plotted to show the time taken to move from one regimen to aNother, while sunburst plots were used to illustrate the sequence in which regimens were introduced. Results About 10% and 35% of cohort members had moved to a second regimen 1 year and 5 years, respectively, after initiating metformin moNotherapy; the majority received a regimen recommended by New Zealand treatment guidelines (mostly metformin and a sulphonylurea). Of those who started a recommended second regimen, 37% and 67% had moved to a third regimen after 1 and 5 years, respectively; the corresponding proportions for those who started an √¢ ‚Ç¨ other' (Not listed as recommended) second regimen were 53% and 75%. Most of those who received a third regimen after a recommended second regimen were dispensed an √¢ ‚Ç¨ other' third regimen. Of those who moved to a third regimen from an √¢ ‚Ç¨ other' second regimen, similar proportions received recommended and √¢ ‚Ç¨ other' third regimens. Conclusions Real-world type 2 diabetes treatment patterns in New Zealand are complex and Not always consistent with guidelines. mention adverse drug effects? Please answer Yes or No.\\nAnswer:\", ' No')[0]\n",
      ")\n",
      "Running loglikelihood requests\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (2273 > 2048). Running this sequence through the model will result in indexing errors\n",
      "100%|██████████| 4348/4348 [15:19<00:00,  4.73it/s]\n"
     ]
    }
   ],
   "source": [
    "results_acc = evaluator.simple_evaluate(\n",
    "        model=\"hf-causal\",\n",
    "        model_args=\" pretrained=\" + model_path_new,\n",
    "        tasks=[\"default\"],\n",
    "        num_fewshot=0,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ccc05940",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device 'cuda'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset json (/home/jovyan/.cache/huggingface/datasets/json/icsr-2e3e9176217a300b/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)\n",
      "100%|██████████| 1/1 [00:00<00:00, 299.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Task: icsr-perplexity; number of docs: 2440\n",
      "Task: icsr-perplexity; document 0; context prompt (starting on next line):\n",
      "\n",
      "(end of prompt on previous line)\n",
      "Requests: Req_loglikelihood_rolling(\"Objectives The aims of this study were to describe the following: (1) the time to change of therapy in patients with type 2 diabetes who had initiated metformin moNotherapy as first-line treatment and (2) the sequence in which subsequent therapeutic regimens were introduced. Design Cohort study. Setting National study based on linked data from the New Zealand Ministry of Health's National Collections of health and pharmaceutical dispensing data. Participants People with type 2 diabetes mellitus who initiated metformin moNotherapy between 1 January 2006 and 30 September 2014 (n=93 874). Primary outcome measures Cumulative incidence curves were plotted to show the time taken to move from one regimen to aNother, while sunburst plots were used to illustrate the sequence in which regimens were introduced. Results About 10% and 35% of cohort members had moved to a second regimen 1 year and 5 years, respectively, after initiating metformin moNotherapy; the majority received a regimen recommended by New Zealand treatment guidelines (mostly metformin and a sulphonylurea). Of those who started a recommended second regimen, 37% and 67% had moved to a third regimen after 1 and 5 years, respectively; the corresponding proportions for those who started an √¢ ‚Ç¨ other' (Not listed as recommended) second regimen were 53% and 75%. Most of those who received a third regimen after a recommended second regimen were dispensed an √¢ ‚Ç¨ other' third regimen. Of those who moved to a third regimen from an √¢ ‚Ç¨ other' second regimen, similar proportions received recommended and √¢ ‚Ç¨ other' third regimens. Conclusions Real-world type 2 diabetes treatment patterns in New Zealand are complex and Not always consistent with guidelines.\",)[None]\n",
      "\n",
      "Running loglikelihood_rolling requests\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|██▍       | 609/2440 [01:44<04:14,  7.18it/s]Token indices sequence length is longer than the specified maximum sequence length for this model (2253 > 2048). Running this sequence through the model will result in indexing errors\n",
      "100%|██████████| 2440/2440 [07:01<00:00,  5.79it/s]\n"
     ]
    }
   ],
   "source": [
    "results_per = evaluator.simple_evaluate(\n",
    "        model=\"hf-causal\",\n",
    "        model_args=\" pretrained=\" + model_path_new,\n",
    "        tasks=[\"icsr-perplexity\"],\n",
    "        num_fewshot=0,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ecb88a64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|Task|Version|Metric|Value |   |Stderr|\n",
      "|----|-------|------|-----:|---|-----:|\n",
      "|icsr|0.1.0  |acc   |0.2307|±  |0.0085|\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(evaluator.make_table(results_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f4183aac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|     Task      |Version|    Metric     | Value |   |Stderr|\n",
      "|---------------|-------|---------------|------:|---|------|\n",
      "|icsr-perplexity|0.1.0  |word_perplexity|33.7934|   |      |\n",
      "|               |       |byte_perplexity| 1.6510|   |      |\n",
      "|               |       |bits_per_byte  | 0.7233|   |      |\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(evaluator.make_table(results_per))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "anaconda3-python-310-lrec [Python]",
   "language": "python",
   "name": "conda-env-anaconda3-python-310-lrec-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
